{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d6eca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "\n",
    "project_root = Path(\".\").resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.clip import load_clip, classify_image\n",
    "from utils.owl import load_owl, detect_with_owl\n",
    "from utils.box_mask import enlarge_box, draw_annotated\n",
    "from utils.gpt_function_calling import GPTInterfaceFC\n",
    "from utils.template import get_template_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0506d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "owl_thresh  = 0.18\n",
    "box_scale   = 1.1\n",
    "font = ImageFont.truetype(\"/Library/fonts/Arial.ttf\", size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a25d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_clip, model_clip = load_clip()\n",
    "processor_owl,  model_owl  = load_owl()\n",
    "env = get_template_env()\n",
    "gpt_fc = GPTInterfaceFC(env, model=\"gpt-4.1\", temperature=0.7)\n",
    "print(\"Models loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4e153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"../pipeline_optimization_dataset/1SIka2FSC_tE6_94GW3GsRvb-Gi5CA8wa__KuÌˆche_Wohnung1.jpg\")\n",
    "image = Image.open(image_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbf650",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_labels_outside = [[\n",
    "    \"a house number\", \"a license plate\", \"person\", \"a face\",\n",
    "    \"a religious symbol\", \"a political symbol\", \"a cat\", \"a dog\",\n",
    "]]\n",
    "text_labels_inside = [[\n",
    "    \"a calendar\", \"a license plate\", \"a paper\", \"person\",\n",
    "    \"a framed picture\", \"a picture\", \"a poster board\",\n",
    "    \"a name\", \"a face\", \"a religious symbol\", \"a political symbol\",\n",
    "    \"a sex toy\", \"a nude image\", \"a cat\", \"a dog\",\n",
    "    \"a mirror\", \"a window\", \"a television\"\n",
    "]]\n",
    "per_label_thresh = {\n",
    "    \"a calendar\": 0.20, \"a paper\": 0.20, \"a house number\": 0.21,\n",
    "    \"a license plate\": 0.19, \"person\": 0.20, \"a framed picture\": 0.22,\n",
    "    \"a picture\": 0.22, \"a poster board\": 0.30, \"a name\": 0.20,\n",
    "    \"a face\": 0.20, \"a religious symbol\": 0.24, \"a political symbol\": 0.20,\n",
    "    \"a sex toy\": 0.23, \"a nude image\": 0.30, \"a cat\": 0.28, \"a dog\": 0.28,\n",
    "    \"a mirror\": 0.30, \"a window\": 0.30, \"a television\": 0.50\n",
    "}\n",
    "default_thresh = owl_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w, h  = image.size\n",
    "inout = classify_image(image, processor_clip, model_clip)\n",
    "labs  = text_labels_inside if inout == \"an indoor scene\" else text_labels_outside\n",
    "print(\"Scene classification:\", inout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67aecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_p, scores, labels = detect_with_owl(image, labs, processor_owl, model_owl, threshold=owl_thresh)\n",
    "raw_boxes = [b.tolist() for b in boxes_p]\n",
    "\n",
    "annotated_pre = draw_annotated(image.copy(), raw_boxes, [float(s.item()) for s in scores], labels, font=font)\n",
    "display(annotated_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43a864",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = [\n",
    "    (b, s, l) for b, s, l in zip(raw_boxes, scores, labels)\n",
    "    if s.item() >= per_label_thresh.get(l, default_thresh)\n",
    "]\n",
    "if kept:\n",
    "    boxes_f, scores_f, labels_f = map(list, zip(*kept))\n",
    "else:\n",
    "    boxes_f, scores_f, labels_f = [], [], []\n",
    "    print(\"No boxes after score filter.\")\n",
    "\n",
    "if boxes_f:\n",
    "    person_or_nude = {\"person\", \"a nude image\"}\n",
    "    always_drop    = {\"a television\", \"a window\", \"a mirror\"}\n",
    "    remove_idx = set()\n",
    "\n",
    "    for i, lab in enumerate(labels_f):\n",
    "        if lab in always_drop:\n",
    "            remove_idx.add(i)\n",
    "\n",
    "    for i, (box_i, lab_i) in enumerate(zip(boxes_f, labels_f)):\n",
    "        if lab_i not in (\"a television\", \"a window\"):\n",
    "            continue\n",
    "        x0, y0, x1, y1 = box_i\n",
    "        Ai = max(0, x1 - x0) * max(0, y1 - y0)\n",
    "        if Ai == 0:\n",
    "            continue\n",
    "        for j, (box_j, lab_j) in enumerate(zip(boxes_f, labels_f)):\n",
    "            if j == i: \n",
    "                continue\n",
    "            x0j, y0j, x1j, y1j = box_j\n",
    "            iw = max(0, min(x1, x1j) - max(x0, x0j))\n",
    "            ih = max(0, min(y1, y1j) - max(y0, y0j))\n",
    "            if iw * ih == 0:\n",
    "                continue\n",
    "            overlap_ratio = (iw * ih) / Ai\n",
    "            if lab_j in person_or_nude:\n",
    "                continue\n",
    "            if overlap_ratio >= 0.20:\n",
    "                remove_idx.add(j)\n",
    "\n",
    "    filtered = [\n",
    "        (b, s, l)\n",
    "        for k, (b, s, l) in enumerate(zip(boxes_f, scores_f, labels_f))\n",
    "        if k not in remove_idx\n",
    "    ]\n",
    "    \n",
    "    if filtered:\n",
    "        boxes_f, scores_f, labels_f = map(list, zip(*filtered))\n",
    "    else:\n",
    "        boxes_f, scores_f, labels_f = [], [], []\n",
    "\n",
    "post_enl = [enlarge_box(box=b, scale=box_scale, img_w=w, img_h=h) for b in boxes_f]\n",
    "\n",
    "annotated_post = draw_annotated(image.copy(), post_enl, [s.item() for s in scores_f], labels_f, font=font)\n",
    "display(annotated_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee217d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_labels = [\n",
    "    {\"label\": l, \"box\": b_enl, \"score\": s.item()}\n",
    "    for b_enl, l, s in zip(post_enl, labels_f, scores_f)\n",
    "]\n",
    "\n",
    "final_dets = []\n",
    "for i, det in enumerate(kept_labels):\n",
    "    lbl, box, score = det[\"label\"], det[\"box\"], det[\"score\"]\n",
    "\n",
    "    if inout == \"an indoor scene\" and lbl in {\"a picture\", \"a framed picture\"}:\n",
    "        crop_img = image.crop(box)\n",
    "        display(crop_img)  \n",
    "\n",
    "        result, usage = gpt_fc.query_inside_fc(\n",
    "            image=crop_img,\n",
    "            label=lbl,\n",
    "            score=score,\n",
    "            box=box,\n",
    "        )\n",
    "        print(\n",
    "            f\"GPT keep={result['keep']}  | \"\n",
    "            f\"prompt={usage['prompt_tokens']}  \"\n",
    "            f\"completion={usage['completion_tokens']}  \"\n",
    "            f\"total={usage['total_tokens']}\"\n",
    "        )\n",
    "        if result[\"keep\"]:\n",
    "            final_dets.append(det)\n",
    "    else:\n",
    "        final_dets.append(det)\n",
    "\n",
    "if final_dets:\n",
    "    boxes_gpt  = [d[\"box\"]   for d in final_dets]\n",
    "    labels_gpt = [d[\"label\"] for d in final_dets]\n",
    "    scores_gpt = [d[\"score\"] for d in final_dets]\n",
    "else:\n",
    "    boxes_gpt, labels_gpt, scores_gpt = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_final = image.copy()\n",
    "annotated_final = draw_annotated(image_final, boxes_gpt, scores_gpt, labels_gpt, font=font)\n",
    "display(annotated_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f62633a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama_object_removal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
