{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d6a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, tempfile\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from IPython.display import display\n",
    "\n",
    "project_root = Path(\".\").resolve().parent  \n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from utils.clip import load_clip, classify_image\n",
    "from utils.owl  import load_owl, detect_with_owl\n",
    "from utils.sam  import load_sam, segment_with_sam, post_process_sam_masks\n",
    "from utils.mask import create_black_white_mask\n",
    "from utils.box_mask import enlarge_box, draw_annotated\n",
    "from utils.opencv_canny import analyze_segmentation_edges\n",
    "from utils.bluring import adaptive_blur\n",
    "from utils.resize   import resize_long_side\n",
    "from lama.runner   import run_lama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d554689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lama_model_dir = project_root / \"lama\" / \"big-lama\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57edf683",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"../pipeline_optimization_dataset/1-_Lxb7M5NPRSYioQ0f8KlvA_Gqr7HW1i___Wohnzimmer1.jpg\") \n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "w, h = image.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "owl_thresh = 0.18\n",
    "target_long = 2048\n",
    "box_scale = 1.1\n",
    "dilations = (0, 10, 30, 55)\n",
    "\n",
    "text_labels_outside = [[\n",
    "    \"a house number\", \"a license plate\", \"person\", \"a face\",\n",
    "    \"a religious symbol\", \"a political symbol\", \"a cat\", \"a dog\",\n",
    "]]\n",
    "text_labels_inside = [[\n",
    "    \"a calendar\", \"a license plate\", \"a paper\", \"person\",\n",
    "    \"a framed picture\", \"a picture\", \"a poster board\",\n",
    "    \"a name\", \"a face\", \"a religious symbol\", \"a political symbol\",\n",
    "    \"a sex toy\", \"a nude image\", \"a cat\", \"a dog\",\n",
    "    \"a mirror\", \"a window\", \"a television\"\n",
    "]]\n",
    "per_label_thresh = {\n",
    "    \"a calendar\": 0.20, \"a paper\": 0.20, \"a house number\": 0.21,\n",
    "    \"a license plate\": 0.19, \"person\": 0.20, \"a framed picture\": 0.22,\n",
    "    \"a picture\": 0.22, \"a poster board\": 0.30, \"a name\": 0.20,\n",
    "    \"a face\": 0.20, \"a religious symbol\": 0.24, \"a political symbol\": 0.20,\n",
    "    \"a sex toy\": 0.23, \"a nude image\": 0.30, \"a cat\": 0.28, \"a dog\": 0.28,\n",
    "    \"a mirror\": 0.30, \"a window\": 0.30, \"a television\": 0.50\n",
    "}\n",
    "default_thresh = owl_thresh\n",
    "always_inpaint_indoor = {\"person\", \"a religious symbol\", \"a political symbol\"}\n",
    "font = ImageFont.truetype(\"/Library/Fonts/Arial.ttf\", size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655da4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor_clip, model_clip = load_clip()\n",
    "processor_owl,  model_owl  = load_owl()\n",
    "processor_sam,  model_sam  = load_sam()\n",
    "print(\"Models loaded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82558f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "inout = classify_image(image, processor_clip, model_clip)\n",
    "labs  = text_labels_inside if inout == \"an indoor scene\" else text_labels_outside\n",
    "print(f\"Scene classification: {inout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a8fc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_p, scores, labels = detect_with_owl(image, labs, processor_owl, model_owl, threshold=owl_thresh)\n",
    "raw_boxes = [b.tolist() for b in boxes_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673af52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_pre = draw_annotated(image.copy(), raw_boxes, [float(s.item()) for s in scores], labels, font=font)\n",
    "display(annot_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b765881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept = [\n",
    "    (b, s, l) for b, s, l in zip(raw_boxes, scores, labels)\n",
    "    if s.item() >= per_label_thresh.get(l, default_thresh)\n",
    "]\n",
    "if kept:\n",
    "    boxes_f, scores_f, labels_f = map(list, zip(*kept))\n",
    "else:\n",
    "    boxes_f, scores_f, labels_f = [], [], []\n",
    "    print(\"no boxes survive score filter; skipping.\\n\")\n",
    "\n",
    "if boxes_f:\n",
    "    person_or_nude = {\"person\", \"a nude image\", \"nude image\"}\n",
    "    always_drop    = {\"a television\", \"a window\", \"a mirror\"}\n",
    "    remove_idx = set(i for i, lab in enumerate(labels_f) if lab in always_drop)\n",
    "\n",
    "    for i, (box_i, lab_i) in enumerate(zip(boxes_f, labels_f)):\n",
    "        if lab_i not in (\"a television\", \"a window\"):\n",
    "            continue\n",
    "        x0, y0, x1, y1 = box_i\n",
    "        A_i = max(0, x1 - x0) * max(0, y1 - y0)\n",
    "        if A_i == 0:\n",
    "            continue\n",
    "        for j, (box_j, lab_j) in enumerate(zip(boxes_f, labels_f)):\n",
    "            if j == i:\n",
    "                continue\n",
    "            x0j, y0j, x1j, y1j = box_j\n",
    "            iw = max(0, min(x1, x1j) - max(x0, x0j))\n",
    "            ih = max(0, min(y1, y1j) - max(y0, y0j))\n",
    "            if iw * ih == 0:\n",
    "                continue\n",
    "            overlap_ratio = (iw * ih) / A_i\n",
    "            if lab_j in person_or_nude:\n",
    "                continue\n",
    "            if overlap_ratio >= 0.20:\n",
    "                remove_idx.add(j)\n",
    "\n",
    "    filtered = [\n",
    "        (b, s, l)\n",
    "        for k, (b, s, l) in enumerate(zip(boxes_f, scores_f, labels_f))\n",
    "        if k not in remove_idx\n",
    "    ]\n",
    "    if filtered:\n",
    "        boxes_f, scores_f, labels_f = map(list, zip(*filtered))\n",
    "    else:\n",
    "        boxes_f, scores_f, labels_f = [], [], []\n",
    "\n",
    "post_enl = [enlarge_box(box=b, scale=box_scale, img_w=w, img_h=h) for b in boxes_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_post = draw_annotated(image.copy(), post_enl, [float(s.item()) for s in scores_f], labels_f, font=font)\n",
    "display(annot_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e3464",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = torch.tensor(post_enl, dtype=torch.float32).unsqueeze(0)\n",
    "outs, ins = segment_with_sam(image, tb, processor_sam, model_sam)\n",
    "masks_from_sam = post_process_sam_masks(outs, processor_sam, ins)[0]\n",
    "sam_bool_masks = [(np.array(m) > 0.5).astype(np.uint8) for m in masks_from_sam]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347e339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_per_d = {}\n",
    "for dpx in dilations:\n",
    "    bw = create_black_white_mask(masks_from_sam, threshold=0.5, combine=True, dilation_px=dpx)\n",
    "    bw_per_d[dpx] = bw\n",
    "    print(f\"Dilation {dpx}px\")\n",
    "    display(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw10_cv = np.array(bw_per_d[10].convert(\"L\"))   \n",
    "mask10_binary = (bw10_cv > 127).astype(np.uint8)\n",
    "contours, _ = cv2.findContours(mask10_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_contours = []\n",
    "image_for_canny = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "is_outdoor = (inout != \"an indoor scene\")\n",
    "\n",
    "for i, cnt in enumerate(contours):\n",
    "    single_mask = np.zeros_like(mask10_binary)\n",
    "    cv2.drawContours(single_mask, [cnt], -1, color=1, thickness=-1)\n",
    "\n",
    "    overlaps = [\n",
    "        np.logical_and(single_mask, sam_bool_masks[j]).sum()\n",
    "        for j in range(len(sam_bool_masks))\n",
    "    ]\n",
    "    obj_idx = int(np.argmax(overlaps)) if overlaps else 0\n",
    "    lbl = labels_f[obj_idx] if labels_f else \"unknown\"\n",
    "\n",
    "    if is_outdoor:\n",
    "        action = \"blur\" if lbl == \"a license plate\" else \"inpaint\"\n",
    "        edge_ratio = float(\"nan\")\n",
    "    else:\n",
    "        if lbl in always_inpaint_indoor:\n",
    "            action, edge_ratio = \"inpaint\", float(\"nan\")\n",
    "        else:\n",
    "            result = analyze_segmentation_edges(image_for_canny,single_mask.astype(bool),margin=60,edge_threshold=0.015)[0]\n",
    "            action = result[\"action\"]\n",
    "            edge_ratio = result[\"edge_ratio\"]\n",
    "\n",
    "    classified_contours.append({\"contour\": cnt, \"action\": action, \"edge_ratio\": edge_ratio})\n",
    "    er_str = f\"{edge_ratio:.3f}\" if edge_ratio == edge_ratio else \"â€”\"\n",
    "    print(f\"Object {i+1}/{len(contours)}: {action} (edge_ratio={er_str})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if post_enl:\n",
    "    annotated_image = image.copy()\n",
    "    draw = ImageDraw.Draw(annotated_image)\n",
    "\n",
    "    for item in classified_contours:\n",
    "        cnt        = item['contour']\n",
    "        action     = item['action']\n",
    "        edge_ratio = item['edge_ratio']\n",
    "\n",
    "        color = \"red\" if action == \"inpaint\" else \"blue\"\n",
    "\n",
    "        pts = [tuple(pt[0]) for pt in cnt]\n",
    "        if len(pts) >= 2:\n",
    "            draw.line(pts + [pts[0]], fill=color, width=3)\n",
    "\n",
    "        x, y, w0, h0 = cv2.boundingRect(cnt)\n",
    "        label_txt = f\"{action}, {edge_ratio:.3f}\" if edge_ratio == edge_ratio else action\n",
    "        text_w = font.getlength(label_txt)\n",
    "        text_h = font.size\n",
    "        pad_x, pad_y = 5, 3\n",
    "        draw.rectangle([x, y - text_h - 2*pad_y, x + text_w + 2*pad_x, y], fill=\"black\")\n",
    "        draw.text((x + pad_x, y - text_h - pad_y), label_txt, font=font, fill=\"white\")\n",
    "\n",
    "    print(\"Action overlay (red=inpaint, blue=blur):\")\n",
    "    display(annotated_image)\n",
    "else:\n",
    "    print(\"No post_enl boxes; skipping overlay.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed45a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_blur_masks  = {d: np.zeros_like(mask10_binary, dtype=np.uint8) for d in dilations}\n",
    "final_inpnt_masks = {d: np.zeros_like(mask10_binary, dtype=np.uint8) for d in dilations}\n",
    "erosion_kernel = np.ones((2 * 10 + 1, 2 * 10 + 1), np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dcec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "needed_dilations = {0, (30 if is_outdoor else 55)}\n",
    "final_blur_masks  = {d: np.zeros_like(mask10_binary, dtype=np.uint8) for d in needed_dilations}\n",
    "final_inpnt_masks = {d: np.zeros_like(mask10_binary, dtype=np.uint8) for d in needed_dilations}\n",
    "\n",
    "for item in classified_contours:\n",
    "    cnt    = item['contour']\n",
    "    action = item['action']\n",
    "\n",
    "    component_10px = np.zeros_like(mask10_binary)\n",
    "    cv2.drawContours(component_10px, [cnt], -1, color=1, thickness=-1)\n",
    "    component_0px  = cv2.erode(component_10px, erosion_kernel, iterations=1)\n",
    "\n",
    "    for dpx in needed_dilations:\n",
    "        if dpx == 0:\n",
    "            final_shape = component_0px\n",
    "        else:\n",
    "            kernel = np.ones((2 * dpx + 1, 2 * dpx + 1), np.uint8)\n",
    "            final_shape = cv2.dilate(component_0px, kernel, iterations=1)\n",
    "\n",
    "        if action == \"blur\" and dpx == 0:\n",
    "            final_blur_masks[dpx] = np.maximum(final_blur_masks[dpx], final_shape)\n",
    "        if action != \"blur\" and dpx in needed_dilations:\n",
    "            final_inpnt_masks[dpx] = np.maximum(final_inpnt_masks[dpx], final_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10191880",
   "metadata": {},
   "outputs": [],
   "source": [
    "paint_dpx = 55 if inout == \"an indoor scene\" else 30\n",
    "blur_dpx  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8306033",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rs = resize_long_side(image, target_long)  \n",
    "final_bgr = cv2.cvtColor(np.array(img_rs), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5ddf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as td:\n",
    "    td = Path(td)\n",
    "    tmp_img  = td / \"img.png\"\n",
    "    tmp_mask = td / \"mask.png\"\n",
    "    cv2.imwrite(str(tmp_img), final_bgr)\n",
    "\n",
    "    pm = final_inpnt_masks.get(paint_dpx, None)\n",
    "    if pm is not None and pm.any():\n",
    "        pm_rs = resize_long_side((pm*255).astype(np.uint8), target_long, nearest=True)\n",
    "        cv2.imwrite(str(tmp_mask), pm_rs)\n",
    "        try:\n",
    "            result_png = run_lama(image_path=tmp_img, mask_path=tmp_mask, model_dir=lama_model_dir, out_dir=td)\n",
    "            result_bgr = cv2.imread(str(result_png), cv2.IMREAD_COLOR)\n",
    "            if result_bgr is not None:\n",
    "                final_bgr = result_bgr\n",
    "            else:\n",
    "                print(\"LaMa output missing; using original resized image.\")\n",
    "        except Exception as e:\n",
    "            print(f\"LaMa failed: {e}; using original resized image.\")\n",
    "    else:\n",
    "        print(\"No inpaint mask to apply.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c728c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = final_blur_masks.get(blur_dpx, None)\n",
    "if bm is not None and bm.any():\n",
    "    bm_rs = resize_long_side((bm*255).astype(np.uint8), target_long, nearest=True)\n",
    "    final_bgr = adaptive_blur(final_bgr, bm_rs, strength=0.6)\n",
    "else:\n",
    "    print(\"No blur mask to apply.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb6f34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final image:\")\n",
    "display(Image.fromarray(cv2.cvtColor(final_bgr, cv2.COLOR_BGR2RGB)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lama_object_removal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
